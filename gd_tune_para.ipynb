{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gd: learning rate and minimal train MSE\n",
      "0.034 0.23300926074058112\n",
      "gd with momentum: learning rate, momentum and minimal train MSE\n",
      "0.08900000000000001 0.2 0.17310653933709938\n",
      "sgd with momentum: learning rate, momentum, learning rate decay and minimal train MSE\n",
      "0.067 0.05 0.95 0.17187237984514658\n",
      "gd: learning rate and minimal train MSE\n",
      "0.023000000000000003 0.17825293946556067\n",
      "gd with momentum: learning rate, momentum and minimal train MSE\n",
      "0.05600000000000001 0.45 0.17211254944432902\n",
      "sgd with momentum: learning rate, momentum, learning rate decay and minimal train MSE\n",
      "0.012 0.05 0.95 0.17187237978593364\n",
      "gd: learning rate and minimal train MSE\n",
      "0.045000000000000005 0.19219173388432584\n",
      "gd with momentum: learning rate, momentum and minimal train MSE\n",
      "0.05600000000000001 0.3 0.18233981469031074\n",
      "sgd with momentum: learning rate, momentum, learning rate decay and minimal train MSE\n",
      "0.023000000000000003 0.3 0.8875 0.17187238010788217\n",
      "gd: learning rate and minimal train MSE\n",
      "0.05600000000000001 0.17374660832881655\n",
      "gd with momentum: learning rate, momentum and minimal train MSE\n",
      "0.05600000000000001 0.2 0.18176076905587768\n",
      "sgd with momentum: learning rate, momentum, learning rate decay and minimal train MSE\n",
      "0.07800000000000001 0.1 0.825 0.17187237988954085\n",
      "gd: learning rate and minimal train MSE\n",
      "0.012 0.24267718021466772\n",
      "gd with momentum: learning rate, momentum and minimal train MSE\n",
      "0.023000000000000003 0.3 0.1732727642934667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/carmen/Project2/gd_tune_para.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/carmen/Project2/gd_tune_para.ipynb#W0sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(eta)):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/carmen/Project2/gd_tune_para.ipynb#W0sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     model \u001b[39m=\u001b[39m GradientDescend(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m\"\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgd\u001b[39m\u001b[39m\"\u001b[39m, learning_rate\u001b[39m=\u001b[39mgamma[i], delta_momentum\u001b[39m=\u001b[39mdelta[j], learning_rate_decay_flag\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, learning_rate_decay\u001b[39m=\u001b[39meta[h])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/carmen/Project2/gd_tune_para.ipynb#W0sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     scores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, z_train, X_test, z_test)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/carmen/Project2/gd_tune_para.ipynb#W0sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     pred_train \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/carmen/Project2/gd_tune_para.ipynb#W0sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     train_score[i, j, h] \u001b[39m=\u001b[39m MSE(pred_train, z_train)\n",
      "File \u001b[0;32m~/Project2/func_autograd.py:161\u001b[0m, in \u001b[0;36mGradientDescend.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthetas\n\u001b[1;32m    160\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthetas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stochastic_gradient_descent(X_train, y_train, X_val, y_val)\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthetas\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Project2/func_autograd.py:136\u001b[0m, in \u001b[0;36mGradientDescend._stochastic_gradient_descent\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    134\u001b[0m     batch_X \u001b[39m=\u001b[39m X_train[batch_indices]\n\u001b[1;32m    135\u001b[0m     batch_y \u001b[39m=\u001b[39m y_train[batch_indices]\n\u001b[0;32m--> 136\u001b[0m     thetas \u001b[39m=\u001b[39m update_fn(batch_X, batch_y, thetas)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate_decay_flag:\n\u001b[1;32m    139\u001b[0m     val_loss \u001b[39m=\u001b[39m mean_squared_error(y_val, np\u001b[39m.\u001b[39mdot(X_val, thetas))\n",
      "File \u001b[0;32m~/Project2/func_autograd.py:52\u001b[0m, in \u001b[0;36mGradientDescend.gradient_descent_step\u001b[0;34m(self, X, y, thetas)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient_descent_step\u001b[39m(\u001b[39mself\u001b[39m, X, y, thetas):\n\u001b[0;32m---> 52\u001b[0m     gradient \u001b[39m=\u001b[39m grad(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcost_function, \u001b[39m2\u001b[39;49m)(X, y, thetas)\n\u001b[1;32m     53\u001b[0m     change \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m*\u001b[39m gradient \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelta_momentum \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchange\n\u001b[1;32m     54\u001b[0m     thetas \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m change\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pythonkurs/lib/python3.10/site-packages/autograd/wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(args[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m argnum)\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m unary_operator(unary_f, x, \u001b[39m*\u001b[39;49mnary_op_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnary_op_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pythonkurs/lib/python3.10/site-packages/autograd/differential_operators.py:28\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m@unary_to_nary\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrad\u001b[39m(fun, x):\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m    Returns a function which computes the gradient of `fun` with respect to\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    positional argument number `argnum`. The returned function takes the same\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m    arguments as `fun`, but returns the gradient instead. The function `fun`\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    should be scalar-valued. The gradient has the same type as the argument.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     vjp, ans \u001b[39m=\u001b[39m _make_vjp(fun, x)\n\u001b[1;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vspace(ans)\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     30\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGrad only applies to real scalar-output functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pythonkurs/lib/python3.10/site-packages/autograd/core.py:10\u001b[0m, in \u001b[0;36mmake_vjp\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_vjp\u001b[39m(fun, x):\n\u001b[1;32m      9\u001b[0m     start_node \u001b[39m=\u001b[39m VJPNode\u001b[39m.\u001b[39mnew_root()\n\u001b[0;32m---> 10\u001b[0m     end_value, end_node \u001b[39m=\u001b[39m  trace(start_node, fun, x)\n\u001b[1;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m end_node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(g): \u001b[39mreturn\u001b[39;00m vspace(x)\u001b[39m.\u001b[39mzeros()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pythonkurs/lib/python3.10/site-packages/autograd/tracer.py:10\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(start_node, fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m trace_stack\u001b[39m.\u001b[39mnew_trace() \u001b[39mas\u001b[39;00m t:\n\u001b[1;32m      9\u001b[0m     start_box \u001b[39m=\u001b[39m new_box(x, t, start_node)\n\u001b[0;32m---> 10\u001b[0m     end_box \u001b[39m=\u001b[39m fun(start_box)\n\u001b[1;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m isbox(end_box) \u001b[39mand\u001b[39;00m end_box\u001b[39m.\u001b[39m_trace \u001b[39m==\u001b[39m start_box\u001b[39m.\u001b[39m_trace:\n\u001b[1;32m     12\u001b[0m         \u001b[39mreturn\u001b[39;00m end_box\u001b[39m.\u001b[39m_value, end_box\u001b[39m.\u001b[39m_node\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pythonkurs/lib/python3.10/site-packages/autograd/wrap_util.py:15\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     subargs \u001b[39m=\u001b[39m subvals(args, \u001b[39mzip\u001b[39m(argnum, x))\n\u001b[0;32m---> 15\u001b[0m \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39;49msubargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Project2/func_autograd.py:48\u001b[0m, in \u001b[0;36mGradientDescend.cost_function\u001b[0;34m(self, X, y, beta)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcost_function\u001b[39m(\u001b[39mself\u001b[39m, X, y, beta):\n\u001b[1;32m     47\u001b[0m     residuals \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X, beta) \u001b[39m-\u001b[39m y\n\u001b[0;32m---> 48\u001b[0m     cost \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msum(residuals \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(X) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlmb \u001b[39m*\u001b[39m (np\u001b[39m.\u001b[39msqrt(np\u001b[39m.\u001b[39msum(beta \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m  \u001b[39m# The sqrt is the squared L2 norm (autograd doesn't support np.linalg.norm arguments like ord=2)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m cost\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pythonkurs/lib/python3.10/site-packages/autograd/tracer.py:44\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m parents \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(box\u001b[39m.\u001b[39m_node \u001b[39mfor\u001b[39;00m _     , box \u001b[39min\u001b[39;00m boxed_args)\n\u001b[1;32m     43\u001b[0m argnums \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(argnum    \u001b[39mfor\u001b[39;00m argnum, _   \u001b[39min\u001b[39;00m boxed_args)\n\u001b[0;32m---> 44\u001b[0m ans \u001b[39m=\u001b[39m f_wrapped(\u001b[39m*\u001b[39;49margvals, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m node \u001b[39m=\u001b[39m node_constructor(ans, f_wrapped, argvals, kwargs, argnums, parents)\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m new_box(ans, trace, node)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pythonkurs/lib/python3.10/site-packages/autograd/tracer.py:48\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m new_box(ans, trace, node)\n\u001b[1;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m f_raw(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pythonkurs/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2162\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum_dispatcher\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2158\u001b[0m                     initial\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2159\u001b[0m     \u001b[39mreturn\u001b[39;00m (a, out)\n\u001b[0;32m-> 2162\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2164\u001b[0m         initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2165\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2166\u001b[0m \u001b[39m    Sum of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2283\u001b[0m \u001b[39m    15\u001b[39;00m\n\u001b[1;32m   2284\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2285\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, _gentype):\n\u001b[1;32m   2286\u001b[0m         \u001b[39m# 2018-02-25, 1.15.0\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import arange\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from func_autograd import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def MSE(y_data, y_model):\n",
    "\tn = np.size(y_model)\n",
    "\ty_data = y_data.reshape(-1,1)\n",
    "\ty_model = y_model.reshape(-1,1)\n",
    "\treturn np.sum((y_data - y_model)**2)/n\n",
    "\n",
    "\n",
    "def generate_data(noise=True, step_size=0.05 , FrankesFunction=True):\n",
    "    # Arrange x and y\n",
    "    x = np.arange(0, 1, step_size)\n",
    "    y = np.arange(0, 1, step_size)\n",
    "    # Create meshgrid of x and y\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    if FrankesFunction:\n",
    "        # Calculate the values for Franke function\n",
    "        z = FrankeFunction(X, Y, noise=noise).flatten()\n",
    "    else:\n",
    "        z = TestFunction(X, Y, noise=noise).flatten()\n",
    "\n",
    "    # Flatten x and y for plotting\n",
    "    x = X.flatten()\n",
    "    y = Y.flatten()\n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "def TestFunction(x, y, noise=False):\n",
    "    if noise: \n",
    "        random_noise = np.random.normal(0, 0.1 , x.shape)\n",
    "    else: \n",
    "        random_noise = 0\n",
    "\n",
    "    return  x**2 + y**2 + 2*x*y + random_noise\n",
    "\n",
    "def FrankeFunction(x, y, noise=False):\n",
    "    if noise: \n",
    "        random_noise = np.random.normal(0, 0.1 , x.shape)\n",
    "    else: \n",
    "        random_noise = 0\n",
    "    \n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4 + random_noise\n",
    "\n",
    "x, y, z = generate_data()\n",
    "X = create_X(x, y, 7)\n",
    "X_train, X_test, z_train, z_test = train_test_split(X, z)\n",
    "\n",
    "\n",
    "# parameters\n",
    "gamma = np.linspace(0.001, 0.1, 10)\n",
    "delta = np.linspace(0.05, 0.5, 10)\n",
    "eta = np.linspace(0.7, 0.95, 5)\n",
    "\n",
    "# OLS\n",
    "beta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ z_train\n",
    "MSE_OLS_train = MSE(X_train @ beta, z_train)\n",
    "MSE_OLS_test = MSE(X_test @ beta, z_test)\n",
    "\n",
    "X_train = X_train[:, 1:3]\n",
    "X_test = X_test[:, 1:3]\n",
    "\n",
    "trials = 10\n",
    "\n",
    "gd_gamma = np.zeros(len(gamma))\n",
    "gd_mom_gamma = np.zeros(len(gamma))\n",
    "gd_mom_delta = np.zeros(len(delta))\n",
    "sgd_mom_gamma = np.zeros(len(gamma))\n",
    "sgd_mom_delta = np.zeros(len(delta))\n",
    "sgd_mom_eta = np.zeros(len(eta))\n",
    "\n",
    "for t in range(trials):\n",
    "    initial_val = np.random.randn(X_train.shape[1],1)\n",
    "\n",
    "    # plain gradient descent with fixed learning rate using analytic expression of gradient\n",
    "    train_score = np.zeros(len(gamma))\n",
    "    for i in range(len(gamma)):\n",
    "        model = GradientDescend(momentum=False, learning_rate=gamma[i])\n",
    "        scores = model.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "        pred_train = model.predict(X_train)\n",
    "        train_score[i] = MSE(pred_train, z_train)\n",
    "\n",
    "    i_min, min = train_score.argmin(), train_score.min()\n",
    "    gd_gamma[i_min] += 1\n",
    "    print(\"gd: learning rate and minimal train MSE\")\n",
    "    print(gamma[i_min], min)\n",
    "\n",
    "    # adding momentum\n",
    "    train_score = np.zeros((len(gamma), len(delta)))\n",
    "    for j in range(len(delta)):\n",
    "        for i in range(len(gamma)):\n",
    "            model = GradientDescend(learning_rate=gamma[i], delta_momentum=delta[j])\n",
    "            scores = model.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "            pred_train = model.predict(X_train)\n",
    "            train_score[i, j] = MSE(pred_train, z_train)\n",
    "        \n",
    "    i_min, min = train_score.argmin(), train_score.min()\n",
    "    k, l=np.shape(train_score)\n",
    "    i_min = np.unravel_index(i_min, shape=[k, l])\n",
    "    gd_mom_gamma[i_min[0]] += 1\n",
    "    gd_mom_delta[i_min[1]] += 1\n",
    "    print(\"gd with momentum: learning rate, momentum and minimal train MSE\")\n",
    "    print(gamma[i_min[0]], delta[i_min[1]], min)\n",
    "\n",
    "\n",
    "    # minibatch sgd, learning schedule\n",
    "    train_score = np.zeros((len(gamma), len(delta), len(eta)))\n",
    "    for j in range(len(delta)):\n",
    "        for i in range(len(gamma)):\n",
    "            for h in range(len(eta)):\n",
    "                model = GradientDescend(optimizer=\"sgd\", method=\"gd\", learning_rate=gamma[i], delta_momentum=delta[j], learning_rate_decay_flag=True, learning_rate_decay=eta[h])\n",
    "                scores = model.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "                pred_train = model.predict(X_train)\n",
    "                train_score[i, j, h] = MSE(pred_train, z_train)\n",
    "                \n",
    "\n",
    "    i_min, min = train_score.argmin(), train_score.min()\n",
    "    k, l, h=np.shape(train_score)\n",
    "    i_min = np.unravel_index(i_min, shape=[k, l, h])\n",
    "    sgd_mom_gamma[i_min[0]] += 1\n",
    "    sgd_mom_delta[i_min[1]] += 1\n",
    "    sgd_mom_eta[i_min[2]] += 1\n",
    "    print(\"sgd with momentum: learning rate, momentum, learning rate decay and minimal train MSE\")\n",
    "    print(gamma[i_min[0]], delta[i_min[1]], eta[i_min[2]], min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'gd_gamma_opt' (float64)\n",
      "Stored 'gd_mom_gamma_opt' (float64)\n",
      "Stored 'sgd_mom_gamma_opt' (float64)\n",
      "Stored 'gd_mom_delta_opt' (float64)\n",
      "Stored 'sgd_mom_delta_opt' (float64)\n",
      "Stored 'sgd_mom_eta_opt' (float64)\n"
     ]
    }
   ],
   "source": [
    "gd_gamma_opt = gamma[gd_gamma.argmax()]\n",
    "gd_mom_gamma_opt = gamma[gd_mom_gamma.argmax()]\n",
    "sgd_mom_gamma_opt = gamma[sgd_mom_gamma.argmax()]\n",
    "\n",
    "gd_mom_delta_opt = delta[gd_mom_delta.argmax()]\n",
    "sgd_mom_delta_opt = delta[sgd_mom_delta.argmax()]\n",
    "\n",
    "sgd_mom_eta_opt = eta[sgd_mom_eta.argmax()]\n",
    "\n",
    "%store gd_gamma_opt\n",
    "%store gd_mom_gamma_opt\n",
    "%store sgd_mom_gamma_opt\n",
    "%store gd_mom_delta_opt\n",
    "%store sgd_mom_delta_opt\n",
    "%store sgd_mom_eta_opt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
