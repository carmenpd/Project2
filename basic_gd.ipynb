{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import arange\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from func_autograd import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def MSE(y_data, y_model):\n",
    "\tn = np.size(y_model)\n",
    "\ty_data = y_data.reshape(-1,1)\n",
    "\ty_model = y_model.reshape(-1,1)\n",
    "\treturn np.sum((y_data - y_model)**2)/n\n",
    "\n",
    "\n",
    "def generate_data(noise=True, step_size=0.05 , FrankesFunction=True):\n",
    "    # Arrange x and y\n",
    "    x = np.arange(0, 1, step_size)\n",
    "    y = np.arange(0, 1, step_size)\n",
    "    # Create meshgrid of x and y\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    if FrankesFunction:\n",
    "        # Calculate the values for Franke function\n",
    "        z = FrankeFunction(X, Y, noise=noise).flatten()\n",
    "    else:\n",
    "        z = TestFunction(X, Y, noise=noise).flatten()\n",
    "\n",
    "    # Flatten x and y for plotting\n",
    "    x = X.flatten()\n",
    "    y = Y.flatten()\n",
    "    \n",
    "    return x, y, z\n",
    "\n",
    "def TestFunction(x, y, noise=False):\n",
    "    if noise: \n",
    "        random_noise = np.random.normal(0, 0.1 , x.shape)\n",
    "    else: \n",
    "        random_noise = 0\n",
    "\n",
    "    return  x**2 + y**2 + 2*x*y + random_noise\n",
    "\n",
    "def FrankeFunction(x, y, noise=False):\n",
    "    if noise: \n",
    "        random_noise = np.random.normal(0, 0.1 , x.shape)\n",
    "    else: \n",
    "        random_noise = 0\n",
    "    \n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4 + random_noise\n",
    "\n",
    "x, y, z = generate_data()\n",
    "X = np.column_stack((x,y))\n",
    "X_train, X_test, z_train, z_test = train_test_split(X, z)\n",
    "\n",
    "# plain gradient descent with fixed learning rate using analytic expression of gradient\n",
    "gd = GradientDescend(momentum=False)\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "gd_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "gd_MSE_test = MSE(pred_test, z_test)\n",
    "\n",
    "# adding momentum\n",
    "gd = GradientDescend()\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "gd_mom_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "gd_mom_MSE_test = MSE(pred_test, z_test)\n",
    "\n",
    "# minibatch sgd, learning schedule\n",
    "# check method\n",
    "gd = GradientDescend(optimizer=\"sgd\", method=\"gd\")\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "sgd_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "sgd_MSE_test = MSE(pred_test, z_test)\n",
    "\n",
    "# adagrad \n",
    "# gd without momentum\n",
    "gd = GradientDescend(momentum=False, method=\"adagrad\")\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "gd_adagrad_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "gd_adagrad_MSE_test = MSE(pred_test, z_test)\n",
    "\n",
    "# gd with momentum\n",
    "gd = GradientDescend(method=\"adagrad\")\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "gd_mom_adagrad_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "gd_mom_adagrad_MSE_test = MSE(pred_test, z_test)\n",
    "\n",
    "# sgd without momentum\n",
    "gd = GradientDescend(optimizer=\"sgd\", momentum=False, method=\"adagrad\")\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "sgd_adagrad_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "sgd_adagrad_MSE_test = MSE(pred_test, z_test)\n",
    "\n",
    "# sgd with momentum\n",
    "gd = GradientDescend(optimizer=\"sgd\", method=\"adagrad\")\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "sgd_mom_adagrad_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "sgd_mom_adagrad_MSE_test = MSE(pred_test, z_test)\n",
    "\n",
    "# rms prop and adam\n",
    "# We chose to only evaluate sgd with momentum for this\n",
    "gd = GradientDescend(optimizer=\"sgd\", method=\"rmsprop\")\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "sgd_mom_rmsprop_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "sgd_mom_rmsprop_MSE_test = MSE(pred_test, z_test)\n",
    "\n",
    "gd = GradientDescend(optimizer=\"sgd\", method=\"adam\")\n",
    "scores = gd.fit(X_train, z_train, X_test, z_test)\n",
    "\n",
    "pred_train = gd.predict(X_train)\n",
    "sgd_mom_adam_MSE_train = MSE(pred_train, z_train)\n",
    "pred_test = gd.predict(X_test)\n",
    "sgd_mom_adam_MSE_test = MSE(pred_test, z_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
